{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Categories: adventure,belles_lettres,editorial,fiction,government,hobbies,humor,learned,lore,mystery,news,religion,reviews,romance,science_fiction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load Brown Corpus\n",
    "from nltk.corpus import brown\n",
    "\"Categories: \" + ','.join( brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories of reuters:\n",
      " acq\n",
      "alum\n",
      "barley\n",
      "bop\n",
      "carcass\n",
      "castor-oil\n",
      "cocoa\n",
      "coconut\n",
      "coconut-oil\n",
      "coffee\n",
      "copper\n",
      "copra-cake\n",
      "corn\n",
      "cotton\n",
      "cotton-oil\n",
      "cpi\n",
      "cpu\n",
      "crude\n",
      "dfl\n",
      "dlr\n",
      "dmk\n",
      "earn\n",
      "fuel\n",
      "gas\n",
      "gnp\n",
      "gold\n",
      "grain\n",
      "groundnut\n",
      "groundnut-oil\n",
      "heat\n",
      "hog\n",
      "housing\n",
      "income\n",
      "instal-debt\n",
      "interest\n",
      "ipi\n",
      "iron-steel\n",
      "jet\n",
      "jobs\n",
      "l-cattle\n",
      "lead\n",
      "lei\n",
      "lin-oil\n",
      "livestock\n",
      "lumber\n",
      "meal-feed\n",
      "money-fx\n",
      "money-supply\n",
      "naphtha\n",
      "nat-gas\n",
      "nickel\n",
      "nkr\n",
      "nzdlr\n",
      "oat\n",
      "oilseed\n",
      "orange\n",
      "palladium\n",
      "palm-oil\n",
      "palmkernel\n",
      "pet-chem\n",
      "platinum\n",
      "potato\n",
      "propane\n",
      "rand\n",
      "rape-oil\n",
      "rapeseed\n",
      "reserves\n",
      "retail\n",
      "rice\n",
      "rubber\n",
      "rye\n",
      "ship\n",
      "silver\n",
      "sorghum\n",
      "soy-meal\n",
      "soy-oil\n",
      "soybean\n",
      "strategic-metal\n",
      "sugar\n",
      "sun-meal\n",
      "sun-oil\n",
      "sunseed\n",
      "tea\n",
      "tin\n",
      "trade\n",
      "veg-oil\n",
      "wheat\n",
      "wpi\n",
      "yen\n",
      "zinc\n"
     ]
    }
   ],
   "source": [
    "# Load Reuters Corpus\n",
    "from nltk.corpus import reuters\n",
    "print(\"Categories of reuters:\\n \" + '\\n'.join(reuters.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name:  hike.n.01\n",
      "POS tag:  n\n",
      "Definetion:  a long walk usually for exercise or pleasure\n",
      "Example:  ['she enjoys a hike in her spare time']\n",
      "\n",
      "\n",
      "Synset name:  rise.n.09\n",
      "POS tag:  n\n",
      "Definetion:  an increase in cost\n",
      "Example:  ['they asked for a 10% rise in rates']\n",
      "\n",
      "\n",
      "Synset name:  raise.n.01\n",
      "POS tag:  n\n",
      "Definetion:  the amount a salary is increased\n",
      "Example:  ['he got a 3% raise', 'he got a wage hike']\n",
      "\n",
      "\n",
      "Synset name:  hike.v.01\n",
      "POS tag:  v\n",
      "Definetion:  increase\n",
      "Example:  ['The landlord hiked up the rents']\n",
      "\n",
      "\n",
      "Synset name:  hike.v.02\n",
      "POS tag:  v\n",
      "Definetion:  walk a long way, as for pleasure or physical exercise\n",
      "Example:  ['We were hiking in Colorado', 'hike the Rockies']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load whe WordNet Corpus\n",
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets('hike'):\n",
    "    print('Synset name: ', syn.name())\n",
    "    print('POS tag: ', syn.pos())\n",
    "    print('Definetion: ', syn.definition())\n",
    "    print('Example: ',syn.examples())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fileids of gutenberg: \n",
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "# 句子切分\n",
    "# Load corpus gutenberg\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "print(\"the fileids of gutenberg: \\n\" + '\\n'.join(gutenberg.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt = gutenberg.raw(fileids='austen-sense.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Sense and Sensibility by Jane Austen 1811]\\n'\n",
      " '\\n'\n",
      " 'CHAPTER 1\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'The family of Dashwood had long been settled in Sussex.',\n",
      " 'Their estate was large, and their residence was at Norland Park,\\n'\n",
      " 'in the centre of their property, where, for many generations,\\n'\n",
      " 'they had lived in so respectable a manner as to engage\\n'\n",
      " 'the general good opinion of their surrounding acquaintance.',\n",
      " 'The late owner of this estate was a single man, who lived\\n'\n",
      " 'to a very advanced age, and who for many years of his life,\\n'\n",
      " 'had a constant companion and housekeeper in his sister.',\n",
      " 'But her death, which happened ten years before his own,\\n'\n",
      " 'produced a great alteration in his home; for to supply\\n'\n",
      " 'her loss, he invited and received into his house the family\\n'\n",
      " 'of his nephew Mr. Henry Dashwood, the legal inheritor\\n'\n",
      " 'of the Norland estate, and the person to whom he intended\\n'\n",
      " 'to bequeath it.',\n",
      " 'In the society of his nephew and niece,\\n'\n",
      " \"and their children, the old Gentleman's days were\\n\"\n",
      " 'comfortably spent.',\n",
      " 'His attachment to them all increased.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "default_st = nltk.sent_tokenize\n",
    "sents = default_st(text=raw_txt)\n",
    "pprint(sents[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileids of sinica_treebanks: \n",
      "parsed\n"
     ]
    }
   ],
   "source": [
    "# Load Chinese corpus sinica_treebanks\n",
    "from nltk.corpus import sinica_treebank\n",
    "print(\"fileids of sinica_treebanks: \\n\" + '\\n'.join(sinica_treebank.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子'], ...]\n"
     ]
    }
   ],
   "source": [
    "cn_text = sinica_treebank.sents()\n",
    "pprint(cn_text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=sent_token_pattern, gaps=True)  # gaps 表示识别pattern 之间的间隙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Sense and Sensibility by Jane Austen 1811]\\n\\nCHAPTER 1\\n\\n\\nThe family of Dashwood had long been settled in Sussex.',\n",
       " 'Their estate was large, and their residence was at Norland Park,\\nin the centre of their property, where, for many generations,\\nthey had lived in so respectable a manner as to engage\\nthe general good opinion of their surrounding acquaintance.',\n",
       " 'The late owner of this estate was a single man, who lived\\nto a very advanced age, and who for many years of his life,\\nhad a constant companion and housekeeper in his sister.',\n",
       " 'But her death, which happened ten years before his own,\\nproduced a great alteration in his home; for to supply\\nher loss, he invited and received into his house the family\\nof his nephew Mr. Henry Dashwood, the legal inheritor\\nof the Norland estate, and the person to whom he intended\\nto bequeath it.',\n",
       " \" In the society of his nephew and niece,\\nand their children, the old Gentleman's days were\\ncomfortably spent.\",\n",
       " ' His attachment to them all increased.',\n",
       " 'The constant attention of Mr. and Mrs.',\n",
       " 'Henry Dashwood\\nto his wishes, which proceeded not merely from interest,\\nbut from goodness of heart, gave him every degree of solid\\ncomfort which his age could receive; and the cheerfulness\\nof the children added a relish to his existence.',\n",
       " '\\nBy a former marriage, Mr. Henry Dashwood had one\\nson: by his present lady, three daughters.',\n",
       " ' The son,\\na steady respectable young man, was amply provided\\nfor by the fortune of his mother, which had been large,\\nand half of which devolved on him on his coming of age.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_split_sents = regex_st.tokenize(raw_txt)\n",
    "regex_split_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词语分割\n",
    "API 包括\n",
    "* word_tokenize\n",
    "* TreebankWordTokenizer\n",
    "* RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "\n",
    "default_wt(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'that',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'he',\n",
       " 'could',\n",
       " \"n't\",\n",
       " 'win',\n",
       " 'the',\n",
       " 'race']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  TreebankWordTokenizer\n",
    "tree_bank_tokenizer = nltk.TreebankWordTokenizer()\n",
    "tree_bank_tokenizer.tokenize(simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本语句和结构\n",
    "* 词性标注\n",
    "* 浅层分析\n",
    "* 基于依存关系的解析\n",
    "* 基于成分结构的解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagger\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:3500]\n",
    "test_data = data[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SequentialBackoffTagger -> defaultTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1454158195372253"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the default tagger\n",
    "dt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regexp tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),\n",
    "    (r'.*ed$', 'VBD'),\n",
    "    (r'.*es$', 'VBZ'),\n",
    "    (r'.*ould$', 'MD'),\n",
    "    (r\".*\\'s$\", 'NN$'),\n",
    "    (r'.*s$', 'NNS'),\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'.*', 'NN')\n",
    "] \n",
    "rt = RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24039113176493368"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NgramTagger\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "unitagger = UnigramTagger(train_data)\n",
    "bitagger = BigramTagger(train_data)\n",
    "tritagger = TrigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Unigram Tagger accuracy: 0.8607803272340013\n",
      "the Bigram Tagger accuracy: 0.13466937748087907\n",
      "the Trigram Tagger accuracy: 0.08064672281924679\n"
     ]
    }
   ],
   "source": [
    "# evaluate the Ngram tagger\n",
    "\n",
    "print('the Unigram Tagger accuracy: %s'%(unitagger.evaluate(test_data)))\n",
    "print('the Bigram Tagger accuracy: %s'%(bitagger.evaluate(test_data)))\n",
    "print('the Trigram Tagger accuracy: %s'%(tritagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined tagger\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tag in taggers:\n",
    "        backoff = tag(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data, [UnigramTagger, BigramTagger, TrigramTagger], rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9094781682641108"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tagger by supervised ml\n",
    "# ClassifierBasedPOSTagger 类中的classifier_builder 中传递相关的分类器\n",
    "from nltk.classify import NaiveBayesClassifier # Naive bayes 分类器\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "naivebayes_tagger = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9306806079969019"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naivebayes_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "maxent_tagger = ClassifierBasedPOSTagger(train=train_data, classifier_builder=MaxentClassifier.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层分析\n",
    "将句子分解成最小的组成部分，然后将它们组合成更高级的短语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (tree.py, line 37)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/liy/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-75-b1a47a55197b>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from pattern3.text import tree\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/liy/anaconda3/lib/python3.6/site-packages/pattern3/text/__init__.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pattern3.text.tree import Tree, Text, Sentence, Slice, Chunk, PNPChunk, Chink, Word, table\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/liy/anaconda3/lib/python3.6/site-packages/pattern3/text/tree.py\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from pattern3.text import tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
